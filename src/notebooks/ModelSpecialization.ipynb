{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the network models to generate clusters\n",
    "- Get a pretrained vgg with some of its training data from ECCV that has code/data (>10classes).\n",
    "- Insert probes at each layer to capture all neuron outputs (activations)\n",
    "- Set a pruning threshold without losing accuracy \n",
    "    - Eg 10 exmaples for one class, keep pruning until prediction barely changes\n",
    "- Run images from each class through the network, grab outputs above a certain threshold as key value (key being the network layer concat neuron number and value is the activation\n",
    "- Check how many in each category is shared by images from the same category\n",
    "- Check how many are shared in all category/some of the categories\n",
    "- Plot histogram of # of neurons shared by n categories (also per layer)\n",
    "- Look at percentage pruning for each class (hopefully >90%)\n",
    "- We do knowledge distillation from each class starting from some base layer (to be varied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "\n",
    "YOLOV3_HOME = '/homes/iws/emazuh/projects/animal_detection/yolov3'\n",
    "sys.path.append(YOLOV3_HOME)\n",
    "LABELING_TOOL_HOME = '/homes/iws/emazuh/projects/animal_detection/labelling_tool'\n",
    "sys.path.append(LABELING_TOOL_HOME)\n",
    "sys.path.append('/homes/iws/emazuh/miniconda3/lib/python3.8/site-packages/torch')\n",
    "\n",
    "from yolov3_models import Darknet\n",
    "from yolov3_utils.datasets import LoadImagesAndLabels\n",
    "from yolov3_utils.utils import non_max_suppression, torch_utils, load_classes\n",
    "from yolov3_utils.parse_config import parse_data_cfg\n",
    "from tracking.Utilities import drawBox, cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='NVIDIA GeForce GTX 1080 Ti', total_memory=11178MB)\n",
      "           device1 _CudaDeviceProperties(name='NVIDIA GeForce GTX 1080 Ti', total_memory=11178MB)\n",
      "\n",
      "Reading annotations from /local1/emazuh/elk/yolo/yolov3/data/animals/output/test_inv.txt\n",
      "Sorting image files...\n",
      "Found 99 sequences with 120 annotations\n"
     ]
    }
   ],
   "source": [
    "img_size = 416\n",
    "batch_size = 1\n",
    "WEIGHTS_PATH = '/local1/emazuh/elk/yolo/yolov3/weights/acamp4k8_tiny'\n",
    "net_cfg = YOLOV3_HOME + '/cfg/elk-tiny-yolov3.cfg'# '/cfg/elk_yolov3.cfg'\n",
    "weights = WEIGHTS_PATH + '/best_99.pt' #'/best_359.pt'\n",
    "test_path = '/local1/emazuh/elk/yolo/yolov3/data/animals/output/test_inv.txt'\n",
    "\n",
    "device = torch_utils.select_device()\n",
    "model = Darknet(net_cfg, img_size).to(device)\n",
    "model.load_state_dict(torch.load(weights, map_location=device)['model'])\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "model.eval()\n",
    "dataset = LoadImagesAndLabels(test_path, img_size, batch_size, rect=False, sort_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/local1/emazuh/elk/yolo/yolov3/data/animals/elk/elk_10_w/image000241.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = [\"bear\", \"moose\", \"coyote\", \"deer\", \"elk\", \"bison\"]\n",
    "exs = {animal: [] for animal in animals}\n",
    "for i in range(120):\n",
    "    label = dataset[i][3]\n",
    "    a = label.split('/')[-3]\n",
    "    exs[a].append(dataset[i][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='NVIDIA GeForce GTX 1080 Ti', total_memory=11178MB)\n",
      "           device1 _CudaDeviceProperties(name='NVIDIA GeForce GTX 1080 Ti', total_memory=11178MB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch_utils.select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module Sequential(\n",
      "  (yolo_16): YOLOLayer()\n",
      ")\n",
      "module Sequential(\n",
      "  (yolo_23): YOLOLayer()\n",
      ")\n",
      "module Sequential(\n",
      "  (yolo_16): YOLOLayer()\n",
      ")\n",
      "module Sequential(\n",
      "  (yolo_23): YOLOLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "animal = 'elk'\n",
    "for i in range(2):\n",
    "    _, _, probe = model(exs[animal][i][0].unsqueeze(0).to(device))\n",
    "    outputs.append(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o, p, probe = model(exs['elk'][0][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bear_outputs = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_act_1 = bear_outputs[0][10]['activation']\n",
    "b_act_2 = bear_outputs[1][10]['activation']\n",
    "b_flat_act_1 = b_act_1.reshape(-1)\n",
    "b_flat_act_2 = b_act_2.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.78176, device='cuda:0', grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_flat_act_1.dot(b_flat_act_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17112379807692307"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((b_flat_act_1 > 1e-32)*(b_flat_act_2 > 1e-32)).cpu().detach().numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20088295118343194"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((b_flat_act_1 > 1e-32)*(flat_act_2 > 1e-32)).cpu().detach().numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elk_outputs = outputs\n",
    "# first only compare activations on first layer\n",
    "act_1 = elk_outputs[0][10]['activation']\n",
    "act_2 = elk_outputs[1][10]['activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_act_1 = act_1.reshape(-1)\n",
    "flat_act_2 = act_2.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.25773, device='cuda:0', grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_act_1.dot(flat_act_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.25773, device='cuda:0', grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_act_1.dot(flat_act_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19371764053254437"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((flat_act_1 > 1e-32)*(flat_act_2 > 1e-32)).cpu().detach().numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49074074074074076"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(probe[0]['params'][0][1].cpu().detach().numpy() > 0.0000001).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 416, 416)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe[0]['activation'].cpu().detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4147154678254438"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(probe[0]['activation'].cpu().detach().numpy() > 0.0000001).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = probe[0]['activation']#.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each example\n",
    "#  for each layer\n",
    "#.  flatten activation (neuron_name = layer_flatid)\n",
    "act.resize(act.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_act = act.reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_d = flat_act.dot(flat_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6234343.50000, device='cuda:0', grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider gpu kmeans \n",
    "# https://github.com/subhadarship/kmeans_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.58528, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(flat_act < 1e-32).sum()/flat_act.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for two examples, see where overlap of zeros is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
